---
phase: 11-gdpr-compliance
plan: 03
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - supabase/migrations/121_gdpr_export_processing.sql
  - src/services/gdprService.js
autonomous: true

must_haves:
  truths:
    - "Export processing RPC generates JSON and stores in data_export_requests"
    - "Service function calls RPC and handles file URL for download"
    - "Presigned URL generated with 7-day expiration"
    - "Export status updated through pending -> processing -> completed"
  artifacts:
    - path: "supabase/migrations/121_gdpr_export_processing.sql"
      provides: "process_data_export RPC function"
      contains: "CREATE OR REPLACE FUNCTION process_data_export"
    - path: "src/services/gdprService.js"
      provides: "processExportRequest function"
      exports: ["processExportRequest"]
  key_links:
    - from: "src/services/gdprService.js"
      to: "process_data_export RPC"
      via: "supabase.rpc call"
      pattern: "supabase\\.rpc\\('process_data_export"
---

<objective>
Create export processing RPC and extend gdprService for export generation

Purpose: Complete the data export flow by processing pending requests, generating JSON files, and providing download URLs. This implements GDPR Article 20 data portability.

Output: Migration with processing RPC and updated gdprService with export handling
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/11-gdpr-compliance/11-RESEARCH.md

# Depends on 11-01 for data collection RPC
# Will be executed after 11-01 completes

# Existing service
@src/services/gdprService.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create export processing RPC</name>
  <files>supabase/migrations/121_gdpr_export_processing.sql</files>
  <action>
Create migration with process_data_export(p_request_id UUID) function.

This function:
1. Gets the export request and validates status is 'pending'
2. Updates status to 'processing' with started_at
3. Calls collect_user_export_data(user_id) from 11-01
4. Stores the export JSON in a temporary location (data_export_requests.export_data JSONB column - add if needed)
5. Updates status to 'completed' with completed_at and expires_at (7 days)
6. Returns the export data for the caller to upload to S3

```sql
-- Add export_data column if not exists
ALTER TABLE data_export_requests
ADD COLUMN IF NOT EXISTS export_data JSONB;

CREATE OR REPLACE FUNCTION process_data_export(p_request_id UUID)
RETURNS JSONB
LANGUAGE plpgsql
SECURITY DEFINER
AS $$
DECLARE
  v_request RECORD;
  v_export_data JSONB;
BEGIN
  -- Get and lock the request
  SELECT * INTO v_request
  FROM data_export_requests
  WHERE id = p_request_id
  FOR UPDATE;

  IF v_request IS NULL THEN
    RETURN jsonb_build_object('success', false, 'error', 'Request not found');
  END IF;

  IF v_request.status NOT IN ('pending', 'processing') THEN
    RETURN jsonb_build_object('success', false, 'error', 'Request already processed');
  END IF;

  -- Mark as processing
  UPDATE data_export_requests
  SET status = 'processing',
      started_at = NOW()
  WHERE id = p_request_id;

  -- Collect all user data using 11-01's function
  SELECT collect_user_export_data(v_request.user_id) INTO v_export_data;

  -- Store export data and mark as ready
  UPDATE data_export_requests
  SET status = 'completed',
      export_data = v_export_data,
      completed_at = NOW(),
      expires_at = NOW() + INTERVAL '7 days',
      file_size_bytes = octet_length(v_export_data::text)
  WHERE id = p_request_id;

  RETURN jsonb_build_object(
    'success', true,
    'requestId', p_request_id,
    'userId', v_request.user_id,
    'exportData', v_export_data,
    'expiresAt', (NOW() + INTERVAL '7 days')::text
  );
END;
$$;
```

Also add function to get pending exports:
```sql
CREATE OR REPLACE FUNCTION get_pending_exports()
RETURNS TABLE (
  id UUID,
  user_id UUID,
  format TEXT,
  requested_at TIMESTAMPTZ
)
LANGUAGE sql
STABLE
SECURITY DEFINER
AS $$
  SELECT id, user_id, format, requested_at
  FROM data_export_requests
  WHERE status = 'pending'
  ORDER BY requested_at ASC
  LIMIT 10;
$$;
```

Grant both functions to service_role.
  </action>
  <verify>
Migration file exists with:
- export_data column added to data_export_requests
- process_data_export function
- get_pending_exports function
- Both granted to service_role
  </verify>
  <done>
Export processing RPC created that collects data and stores in database, ready for download
  </done>
</task>

<task type="auto">
  <name>Task 2: Add export download function to gdprService</name>
  <files>src/services/gdprService.js</files>
  <action>
Extend gdprService.js with function to get completed export data.

Add to the Data Export section after existing functions:

```javascript
/**
 * Get completed export data for download
 * @param {string} requestId - Export request ID
 * @returns {Promise<{success: boolean, data?: object, error?: string}>}
 */
export async function getExportData(requestId) {
  try {
    const { data, error } = await supabase
      .from('data_export_requests')
      .select('export_data, status, expires_at')
      .eq('id', requestId)
      .single();

    if (error) {
      return { success: false, error: error.message };
    }

    if (data.status !== 'completed') {
      return { success: false, error: 'Export not yet ready' };
    }

    if (new Date(data.expires_at) < new Date()) {
      return { success: false, error: 'Export has expired' };
    }

    return { success: true, data: data.export_data };
  } catch (error) {
    logger.error('Failed to get export data:', { error, requestId });
    return { success: false, error: error.message };
  }
}

/**
 * Download export as JSON file
 * @param {string} requestId - Export request ID
 */
export async function downloadExportAsFile(requestId) {
  const result = await getExportData(requestId);

  if (!result.success) {
    throw new Error(result.error);
  }

  const blob = new Blob(
    [JSON.stringify(result.data, null, 2)],
    { type: 'application/json' }
  );

  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = `bizscreen-data-export-${new Date().toISOString().split('T')[0]}.json`;
  document.body.appendChild(a);
  a.click();
  document.body.removeChild(a);
  URL.revokeObjectURL(url);

  return { success: true };
}
```

Add import for logger at top if not present:
```javascript
import { createScopedLogger } from './loggingService.js';
const logger = createScopedLogger('gdprService');
```

Update default export to include new functions.
  </action>
  <verify>
gdprService.js contains:
- getExportData function
- downloadExportAsFile function
- logger import and initialization
- Updated default export
  </verify>
  <done>
gdprService extended with export download capability that retrieves stored JSON and triggers browser download
  </done>
</task>

</tasks>

<verification>
- [ ] Migration 121_gdpr_export_processing.sql exists
- [ ] export_data JSONB column added to data_export_requests
- [ ] process_data_export RPC collects data using collect_user_export_data
- [ ] get_pending_exports RPC returns pending requests
- [ ] gdprService.js has getExportData function
- [ ] gdprService.js has downloadExportAsFile function
- [ ] Status flow: pending -> processing -> completed works
- [ ] 7-day expiration set on completed exports
</verification>

<success_criteria>
- Export request can be processed and data collected into database
- Completed export data can be retrieved and downloaded as JSON file
- Export expiration enforced (returns error if expired)
- File download triggers with proper filename
</success_criteria>

<output>
After completion, create `.planning/phases/11-gdpr-compliance/11-03-SUMMARY.md`
</output>
