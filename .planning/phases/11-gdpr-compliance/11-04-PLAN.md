---
phase: 11-gdpr-compliance
plan: 04
type: execute
wave: 2
depends_on: ["11-02"]
files_modified:
  - src/services/gdprDeletionService.js
autonomous: true

must_haves:
  truths:
    - "Service deletes files from S3 using DeleteObjectsCommand"
    - "Service deletes files from Cloudinary using Admin API"
    - "URLs parsed to determine S3 vs Cloudinary storage"
    - "Batch deletion handles large numbers of files"
  artifacts:
    - path: "src/services/gdprDeletionService.js"
      provides: "External media deletion functions"
      exports: ["deleteS3Files", "deleteCloudinaryFiles", "deleteUserMediaFiles"]
  key_links:
    - from: "deleteUserMediaFiles"
      to: "S3 and Cloudinary APIs"
      via: "AWS SDK and Cloudinary Admin API"
      pattern: "(DeleteObjectsCommand|delete_resources)"
---

<objective>
Create service for deleting user media files from S3 and Cloudinary

Purpose: GDPR Article 17 requires deletion to propagate to third-party processors. When account is deleted, media files must be removed from cloud storage providers.

Output: New service file with S3 and Cloudinary deletion functions
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/11-gdpr-compliance/11-RESEARCH.md

# Existing cloud services
@src/services/cloudinaryService.js
@src/services/s3UploadService.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create gdprDeletionService with URL parsing</name>
  <files>src/services/gdprDeletionService.js</files>
  <action>
Create new service file for GDPR deletion operations.

```javascript
/**
 * GDPR Deletion Service
 *
 * Handles deletion of user media files from external storage providers.
 * Required for GDPR Article 17 compliance - right to erasure must propagate
 * to all third-party data processors.
 */

import { createScopedLogger } from './loggingService.js';

const logger = createScopedLogger('gdprDeletionService');

/**
 * Parse media URL to determine storage provider and extract key/public_id
 * @param {string} url - Media URL
 * @returns {{ provider: 'S3'|'cloudinary'|'unknown', key: string|null }}
 */
export function parseMediaUrl(url) {
  if (!url) return { provider: 'unknown', key: null };

  // Cloudinary URL pattern: https://res.cloudinary.com/cloud_name/image/upload/v1234/folder/filename.ext
  if (url.includes('cloudinary.com')) {
    const match = url.match(/\/upload\/(?:v\d+\/)?(.+?)(?:\.\w+)?$/);
    return {
      provider: 'cloudinary',
      key: match ? match[1] : null,
    };
  }

  // S3 URL patterns:
  // https://bucket.s3.region.amazonaws.com/key
  // https://s3.region.amazonaws.com/bucket/key
  if (url.includes('s3.') && url.includes('amazonaws.com')) {
    try {
      const urlObj = new URL(url);
      // Remove leading slash from pathname
      const key = urlObj.pathname.startsWith('/')
        ? urlObj.pathname.slice(1)
        : urlObj.pathname;
      return {
        provider: 's3',
        key,
      };
    } catch {
      return { provider: 's3', key: null };
    }
  }

  return { provider: 'unknown', key: null };
}

/**
 * Separate media URLs by provider
 * @param {Array<{url: string, thumbnailUrl?: string}>} mediaItems
 * @returns {{ s3Keys: string[], cloudinaryPublicIds: string[] }}
 */
export function categorizeMediaUrls(mediaItems) {
  const s3Keys = [];
  const cloudinaryPublicIds = [];

  for (const item of mediaItems) {
    // Main URL
    const mainParsed = parseMediaUrl(item.url);
    if (mainParsed.provider === 's3' && mainParsed.key) {
      s3Keys.push(mainParsed.key);
    } else if (mainParsed.provider === 'cloudinary' && mainParsed.key) {
      cloudinaryPublicIds.push(mainParsed.key);
    }

    // Thumbnail URL (if different from main)
    if (item.thumbnailUrl && item.thumbnailUrl !== item.url) {
      const thumbParsed = parseMediaUrl(item.thumbnailUrl);
      if (thumbParsed.provider === 's3' && thumbParsed.key) {
        s3Keys.push(thumbParsed.key);
      } else if (thumbParsed.provider === 'cloudinary' && thumbParsed.key) {
        cloudinaryPublicIds.push(thumbParsed.key);
      }
    }
  }

  return {
    s3Keys: [...new Set(s3Keys)], // Dedupe
    cloudinaryPublicIds: [...new Set(cloudinaryPublicIds)],
  };
}
```
  </action>
  <verify>
File exists with:
- parseMediaUrl function
- categorizeMediaUrls function
- Handles both S3 and Cloudinary URL patterns
  </verify>
  <done>
URL parsing utilities created that correctly identify S3 vs Cloudinary URLs and extract keys/public_ids
  </done>
</task>

<task type="auto">
  <name>Task 2: Add S3 and Cloudinary deletion functions</name>
  <files>src/services/gdprDeletionService.js</files>
  <action>
Add deletion functions to gdprDeletionService.js:

```javascript
/**
 * Delete files from S3
 * Note: This requires server-side execution with AWS credentials.
 * For client-side, this will call an API endpoint.
 *
 * @param {string[]} keys - S3 object keys to delete
 * @returns {Promise<{success: boolean, deleted: number, errors: string[]}>}
 */
export async function deleteS3Files(keys) {
  if (!keys || keys.length === 0) {
    return { success: true, deleted: 0, errors: [] };
  }

  const errors = [];

  try {
    // Call server API endpoint for S3 deletion
    // S3 DeleteObjects allows max 1000 per request
    const API_BASE = import.meta.env.VITE_API_URL || '';
    const chunks = chunkArray(keys, 1000);
    let totalDeleted = 0;

    for (const chunk of chunks) {
      const response = await fetch(`${API_BASE}/api/gdpr/delete-s3`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ keys: chunk }),
      });

      if (!response.ok) {
        const error = await response.json().catch(() => ({ error: 'Unknown error' }));
        errors.push(error.error || `Failed to delete ${chunk.length} files`);
      } else {
        const result = await response.json();
        totalDeleted += result.deleted || chunk.length;
      }
    }

    logger.info('S3 deletion completed', { totalDeleted, errors: errors.length });
    return { success: errors.length === 0, deleted: totalDeleted, errors };
  } catch (error) {
    logger.error('S3 deletion failed', { error: error.message, keyCount: keys.length });
    return { success: false, deleted: 0, errors: [error.message] };
  }
}

/**
 * Delete files from Cloudinary
 * Note: Requires server-side execution with Admin API credentials.
 *
 * @param {string[]} publicIds - Cloudinary public IDs to delete
 * @returns {Promise<{success: boolean, deleted: number, errors: string[]}>}
 */
export async function deleteCloudinaryFiles(publicIds) {
  if (!publicIds || publicIds.length === 0) {
    return { success: true, deleted: 0, errors: [] };
  }

  const errors = [];

  try {
    // Call server API endpoint for Cloudinary deletion
    // Admin API allows max 100 per request
    const API_BASE = import.meta.env.VITE_API_URL || '';
    const chunks = chunkArray(publicIds, 100);
    let totalDeleted = 0;

    for (const chunk of chunks) {
      const response = await fetch(`${API_BASE}/api/gdpr/delete-cloudinary`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ publicIds: chunk }),
      });

      if (!response.ok) {
        const error = await response.json().catch(() => ({ error: 'Unknown error' }));
        errors.push(error.error || `Failed to delete ${chunk.length} files`);
      } else {
        const result = await response.json();
        totalDeleted += result.deleted || chunk.length;
      }
    }

    logger.info('Cloudinary deletion completed', { totalDeleted, errors: errors.length });
    return { success: errors.length === 0, deleted: totalDeleted, errors };
  } catch (error) {
    logger.error('Cloudinary deletion failed', { error: error.message, idCount: publicIds.length });
    return { success: false, deleted: 0, errors: [error.message] };
  }
}

/**
 * Chunk array into smaller arrays
 */
function chunkArray(array, size) {
  const chunks = [];
  for (let i = 0; i < array.length; i += size) {
    chunks.push(array.slice(i, i + size));
  }
  return chunks;
}

/**
 * Delete all media files for a user
 * @param {Array<{url: string, thumbnailUrl?: string}>} mediaItems
 * @returns {Promise<{success: boolean, s3: object, cloudinary: object}>}
 */
export async function deleteUserMediaFiles(mediaItems) {
  const { s3Keys, cloudinaryPublicIds } = categorizeMediaUrls(mediaItems);

  logger.info('Starting media deletion', {
    s3Count: s3Keys.length,
    cloudinaryCount: cloudinaryPublicIds.length,
  });

  const [s3Result, cloudinaryResult] = await Promise.all([
    deleteS3Files(s3Keys),
    deleteCloudinaryFiles(cloudinaryPublicIds),
  ]);

  const success = s3Result.success && cloudinaryResult.success;

  logger.info('Media deletion complete', {
    success,
    s3Deleted: s3Result.deleted,
    cloudinaryDeleted: cloudinaryResult.deleted,
    errors: [...s3Result.errors, ...cloudinaryResult.errors],
  });

  return {
    success,
    s3: s3Result,
    cloudinary: cloudinaryResult,
  };
}

export default {
  parseMediaUrl,
  categorizeMediaUrls,
  deleteS3Files,
  deleteCloudinaryFiles,
  deleteUserMediaFiles,
};
```
  </action>
  <verify>
File contains:
- deleteS3Files function with chunking
- deleteCloudinaryFiles function with chunking
- deleteUserMediaFiles orchestrator function
- chunkArray helper
  </verify>
  <done>
External media deletion service complete with S3 and Cloudinary support, batch processing, and error handling
  </done>
</task>

</tasks>

<verification>
- [ ] src/services/gdprDeletionService.js exists
- [ ] parseMediaUrl correctly identifies S3 and Cloudinary URLs
- [ ] categorizeMediaUrls separates URLs by provider
- [ ] deleteS3Files handles batches of 1000
- [ ] deleteCloudinaryFiles handles batches of 100
- [ ] deleteUserMediaFiles orchestrates both deletion types
- [ ] Proper error handling and logging
- [ ] Returns success/failure with deleted counts
</verification>

<success_criteria>
- Media URLs correctly parsed to identify S3 vs Cloudinary
- deleteUserMediaFiles deletes all user media from both providers
- Batch processing handles large media libraries
- Errors logged but don't break the flow (best-effort deletion)
</success_criteria>

<output>
After completion, create `.planning/phases/11-gdpr-compliance/11-04-SUMMARY.md`
</output>
