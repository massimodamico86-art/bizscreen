---
phase: 11-gdpr-compliance
plan: 09
type: execute
wave: 1
depends_on: []
files_modified:
  - src/api/gdpr/process-exports.js
  - src/api/gdpr/process-deletions.js
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Export ready email is sent when export processing completes"
    - "S3 files are deleted via /api/gdpr/delete-s3 endpoint"
    - "Cloudinary files are deleted via /api/gdpr/delete-cloudinary endpoint"
  artifacts:
    - path: "src/api/gdpr/process-exports.js"
      provides: "Email notification integration"
      contains: "sendExportReadyEmail"
    - path: "src/api/gdpr/process-deletions.js"
      provides: "External media deletion via API endpoints"
      contains: "fetch.*delete-s3"
  key_links:
    - from: "src/api/gdpr/process-exports.js"
      to: "src/services/emailService.js"
      via: "dynamic import and function call"
      pattern: "sendExportReadyEmail\\("
    - from: "src/api/gdpr/process-deletions.js"
      to: "/api/gdpr/delete-s3"
      via: "fetch POST with bearer token"
      pattern: "fetch.*delete-s3"
    - from: "src/api/gdpr/process-deletions.js"
      to: "/api/gdpr/delete-cloudinary"
      via: "fetch POST with bearer token"
      pattern: "fetch.*delete-cloudinary"
---

<objective>
Wire placeholder functions in GDPR processing endpoints to actual implementations

Purpose: Close GDPR compliance gaps by connecting process-exports.js to emailService and process-deletions.js to the S3/Cloudinary deletion endpoints. These are the two blocking issues preventing GDPR-02 and GDPR-05 compliance.

Output: Fully functional GDPR processing endpoints that send real emails and delete real files
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-gdpr-compliance/11-VERIFICATION.md

# Existing implementations to wire to
@src/services/emailService.js
@src/api/gdpr/delete-s3.js
@src/api/gdpr/delete-cloudinary.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire export email notification</name>
  <files>src/api/gdpr/process-exports.js</files>
  <action>
Replace the placeholder sendExportReadyEmail function (lines 82-99) with actual email sending.

Current placeholder (line 95):
```javascript
console.log(`Export ready notification for ${profile.email}, request ${requestId}`);
```

Replace with dynamic import and actual call to emailService:
1. Remove the standalone sendExportReadyEmail function (lines 82-99)
2. Replace the function call at line 62 with inline implementation:

```javascript
// After successful export processing (inside the for loop, after line 60)
if (error) {
  results.push({ id: exportRequest.id, success: false, error: error.message });
} else {
  // Send notification email
  try {
    const { data: profile } = await supabaseAdmin
      .from('profiles')
      .select('email')
      .eq('id', exportRequest.user_id)
      .single();

    if (profile?.email) {
      // Dynamic import to avoid bundling issues in API routes
      const { sendExportReadyEmail } = await import('../../services/emailService.js');

      const appUrl = process.env.VITE_APP_URL || process.env.APP_URL || 'https://app.bizscreen.com';
      await sendExportReadyEmail({
        to: profile.email,
        downloadUrl: `${appUrl}/settings/privacy?export=${exportRequest.id}`,
        expiresAt: new Date(Date.now() + 7 * 24 * 60 * 60 * 1000).toISOString(),
      });
    }
  } catch (emailError) {
    // Email is non-critical, log but don't fail the export
    console.error('Failed to send export ready email:', emailError.message);
  }
  results.push({ id: exportRequest.id, success: true });
}
```

Key points:
- Use dynamic import to avoid bundling emailService dependencies
- Get email from profiles table (already have supabaseAdmin client)
- Fire-and-forget pattern - email failure doesn't fail the export
- Build download URL from APP_URL env var
- Set expiration 7 days from now per RESEARCH.md recommendation
  </action>
  <verify>
Verify the file:
1. `grep -n "sendExportReadyEmail" src/api/gdpr/process-exports.js` shows import and call
2. `grep -n "console.log.*Export ready" src/api/gdpr/process-exports.js` returns no matches
3. File has no syntax errors: `node --check src/api/gdpr/process-exports.js` (or import check)
  </verify>
  <done>
process-exports.js calls emailService.sendExportReadyEmail with proper parameters (to, downloadUrl, expiresAt) when export completes successfully
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire external media deletion</name>
  <files>src/api/gdpr/process-deletions.js</files>
  <action>
Replace the placeholder deleteExternalMedia function (lines 130-159) with actual API calls to delete-s3.js and delete-cloudinary.js endpoints.

Current placeholder (lines 155-156):
```javascript
results.s3.deleted = s3Keys.length; // Placeholder
results.cloudinary.deleted = cloudinaryIds.length; // Placeholder
```

Replace deleteExternalMedia function with actual fetch calls:

```javascript
async function deleteExternalMedia(mediaUrls) {
  // Parse and categorize URLs
  const s3Keys = [];
  const cloudinaryIds = [];

  for (const item of mediaUrls) {
    if (item.url?.includes('cloudinary.com')) {
      const match = item.url.match(/\/upload\/(?:v\d+\/)?(.+?)(?:\.\w+)?$/);
      if (match) cloudinaryIds.push(match[1]);
    } else if (item.url?.includes('s3.') && item.url?.includes('amazonaws.com')) {
      try {
        const urlObj = new URL(item.url);
        s3Keys.push(urlObj.pathname.slice(1));
      } catch { /* skip invalid URLs */ }
    }
  }

  const results = { s3: { deleted: 0, errors: [] }, cloudinary: { deleted: 0, errors: [] } };
  const token = process.env.GDPR_API_SECRET;
  const apiBase = process.env.VITE_API_URL || process.env.API_URL || 'http://localhost:3000';

  // Helper to chunk arrays
  const chunkArray = (arr, size) => {
    const chunks = [];
    for (let i = 0; i < arr.length; i += size) {
      chunks.push(arr.slice(i, i + size));
    }
    return chunks;
  };

  // Delete from S3 in batches of 1000 (AWS limit)
  for (const chunk of chunkArray(s3Keys, 1000)) {
    try {
      const response = await fetch(`${apiBase}/api/gdpr/delete-s3`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ keys: chunk }),
      });
      const result = await response.json();
      results.s3.deleted += result.deleted || 0;
      if (result.errors?.length) {
        results.s3.errors.push(...result.errors);
      }
    } catch (error) {
      results.s3.errors.push(error.message);
    }
  }

  // Delete from Cloudinary in batches of 100 (Cloudinary Admin API limit)
  for (const chunk of chunkArray(cloudinaryIds, 100)) {
    try {
      const response = await fetch(`${apiBase}/api/gdpr/delete-cloudinary`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ publicIds: chunk }),
      });
      const result = await response.json();
      results.cloudinary.deleted += result.deleted || 0;
    } catch (error) {
      results.cloudinary.errors.push(error.message);
    }
  }

  return results;
}
```

Key points:
- Reuse existing URL parsing logic (already correct)
- Add chunkArray helper for batch processing
- S3: 1000 per batch per AWS DeleteObjects limit
- Cloudinary: 100 per batch per Admin API limit
- Use GDPR_API_SECRET for authorization (same token all GDPR endpoints use)
- API_URL fallback to localhost for development
- Collect errors but don't throw - best-effort deletion per RESEARCH.md
  </action>
  <verify>
Verify the file:
1. `grep -n "fetch.*delete-s3" src/api/gdpr/process-deletions.js` shows fetch call
2. `grep -n "fetch.*delete-cloudinary" src/api/gdpr/process-deletions.js` shows fetch call
3. `grep -n "Placeholder" src/api/gdpr/process-deletions.js` returns no matches
4. File has no syntax errors
  </verify>
  <done>
process-deletions.js calls /api/gdpr/delete-s3 and /api/gdpr/delete-cloudinary with proper batch sizes (1000 and 100 respectively) and bearer token authorization
  </done>
</task>

</tasks>

<verification>
After both tasks:
1. `grep -rn "console.log.*Export ready" src/api/gdpr/` returns no matches
2. `grep -rn "Placeholder" src/api/gdpr/` returns no matches
3. `grep -n "sendExportReadyEmail" src/api/gdpr/process-exports.js` shows import and call
4. `grep -n "fetch.*delete-" src/api/gdpr/process-deletions.js` shows both S3 and Cloudinary calls
5. `npm run lint src/api/gdpr/` passes (or equivalent syntax check)
</verification>

<success_criteria>
- process-exports.js calls emailService.sendExportReadyEmail when export completes
- process-deletions.js calls /api/gdpr/delete-s3 with batches of max 1000 keys
- process-deletions.js calls /api/gdpr/delete-cloudinary with batches of max 100 publicIds
- No placeholder code remains in either file
- Email and deletion failures are logged but don't break the processing flow
</success_criteria>

<output>
After completion, create `.planning/phases/11-gdpr-compliance/11-09-SUMMARY.md`
</output>
